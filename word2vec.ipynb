{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f7ad5f-1892-4f98-ab65-0ec4bd466ef3",
   "metadata": {},
   "source": [
    "# A brief introduction to word embeddings and neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8db142-ab41-44d2-b010-2e1787e31599",
   "metadata": {},
   "source": [
    "![alt text](w2v_image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b554409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip install plotly\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from IPython.display import IFrame\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff3e774",
   "metadata": {},
   "source": [
    "# Preliminary: matrix multiplication\n",
    "\n",
    "Matrices are arrays of numbers in of $m \\times n$ dimensions, where $m$ and $n$ dimensions ($m$ is the number rows; $n$ the number of columns). These are all examples of matrices:\n",
    "\n",
    "$$A = \\begin{bmatrix}1& 5 & 7 & 23& 6\\end{bmatrix}$$ \n",
    "\\\n",
    "$$B = \\begin{bmatrix}1& 5 \\\\ 7 & 23 \\\\ 6 & 9\\end{bmatrix}$$\n",
    "\\\n",
    "$$C = \\begin{bmatrix}5 \\\\ 7 \\\\ 9\\end{bmatrix}$$\n",
    "\\\n",
    "$$D = \\begin{bmatrix}1& 5 & i\\\\ 7 & 3i & e\\\\ \\pi & 6 & 9\\end{bmatrix} \\ E = \\begin{bmatrix}x^3& x+y & y^2\\\\ \\sqrt{x} & x^2 + y^2 & x^y\\end{bmatrix}$$\n",
    "\n",
    "Linear algebra is the branch of mathematics that deals with matrix operations, and it underwrites much of NLP. For our purposes in learning `word2vec`, we need to know one operation: matrix multiplication. \n",
    "\n",
    "To multiply two matrices, the number of columns of the first matrix must equal the number of rows of the second matrix. That is, they must have dimensions $m \\times n$ and $n \\times p$. The multiplication operation consists of  starting with the first row of the first matrix and multiplying each entry in that row with the corresponding entry in the first column of the second matrix and adding the results. This gives the first entry of the new matrix. This is repeated over all the columns of the seond matrix, giving the entire first row of the new matrix. Then, you move on to the second row of the first matrix and repeat the process; this gives the second row of the new matrix. Once the process has completed for all the rows of the first matrix, the operation is compelete. This is easier to follow with an example:\n",
    "\n",
    "$$\\begin{bmatrix}1& 5 \\\\ 7 & 23 \\\\ 6 & 9\\end{bmatrix} \\times \\begin{bmatrix}4 & 5 & 10 & 9 \\\\ 7 & 2 & 5 & 1 \\end{bmatrix} = \\begin{bmatrix}1\\times4+5\\times7 & 1\\times5+5\\times2 & 1\\times10+5\\times5 & 1\\times9 + 5\\times1 \\\\ 7\\times4+23\\times7 & 7\\times5+23\\times2 & 7\\times10+23\\times5 & 7\\times9 + 23\\times1 \\\\ 6\\times4+9\\times7 & 6\\times5+9\\times2 & 6\\times10+9\\times5 & 6\\times9 + 9\\times1\\end{bmatrix} = \\begin{bmatrix}39 & 15 & 35 & 14 \\\\ 189 & 81 & 185 & 86 \\\\ 87 & 48 & 105 & 63\\end{bmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a6f9e6",
   "metadata": {},
   "source": [
    "# The theory of word embeddings\n",
    "\n",
    "Most present-day NLP applications are based on word embeddings. These are a way of representing words and larger items of text based on how they behave across a corpus. Typically, these behaviours are learned by neural networks, which can then be used to describe or generate language samples. Large Language Models (LLMs) like ChatGPT, GPT3, and T5 were all made possible be the emergence of word embeddings as a way of describing language.\n",
    "\n",
    "The linguistic principle behind word embeddings is what's known as the distributional hypothesis. According to [Sahlgren (2006)](https://www.diva-portal.org/smash/get/diva2:1041938/FULLTEXT01.pdf), this can be stated as the claim that:\n",
    "\n",
    ">there is a correlation between distributional similarity and meaning similarity, which allows us to utilize the former in order to estimate the latter.\n",
    "\n",
    "That is, words that mean the same thing can generally be found in the same contexts as other words that mean the same thing. For example, the words 'cat' and 'feline' are more likely to share contexts with each other than the words 'dog' or 'canine'. Word embedding models seek to represent this common context mathematically, so we can use them with NLP methods.\n",
    "\n",
    "The first word embedding model was `word2vec`, which was developed by [Mikolov et al](https://arxiv.org/abs/1301.3781) in 2013. Though `word2vec` has since been superseded by more sophisticated embedding models, the basic principles that underwrite them are the same. So, how does `word2vec` work?\n",
    "\n",
    "The key insight is that `word2vec` wants to create vector representations of words, such that the vectors for words that share contexts with each other are closer in space. The model starts out by assigning each word a random vector (otherwise called a set of weights). During training on a corpus, the model learns which words are close to each other and adjusts the weights in an iterative process. Usually, the vectors are in 300 dimensions. Why 300? The choice is somewhat abitrary and can be any number, but 300 was found to provide enough paramaters to be empirically useful. In any event 300 parameters gives a lot of degrees of freedom; to quote Jon von Neumann:\n",
    "\n",
    "> with four parameters I can fit an elephant, and with five I can make him wiggle his trunk.\n",
    "\n",
    "In more detail, `word2vec` procedes as follows:\n",
    "\n",
    "1. The corpus is tokenized into words.\n",
    "2. Each word is represented as a $1 \\times V$ matrix using one-hot encoding; this is a vector of size $V$, where $V$ is the vocabulary size.\n",
    "3. A matrix of $V \\times 300$ size is randomly initialised; this is called the hidden layer. Multiplying this vector by the one-hot representation of a word assigns that word one of the 300 place vectors. This vector is its embedding.\n",
    "4. A second random matrix of $300 \\times V$ is created; this is called the output layer. It's the same as the embeddings, but transposed to enable matrix multiplication. \n",
    "5. For a given word, its behaviour relative to every other word can be captured by getting a similarity measure of its embeddding and the output later weights: this gives a single number that measures vector similarity. Doing this for every word in the vocabulary gives a $1 \\times V$ matrix of values.\n",
    "6. This matrix is converted into a probability distribution using the softmax function. This gives the probability of the word co-occurring with every other word in the vocabulary\n",
    "7. In the process of training, the `word2vec` model learns what words are *actually* associated with each other. It uses this knowledge to adjust the word embeddings and output weights to match the actual probability distributions. It does this iteratively until training ends. \n",
    "\n",
    "## Toy example\n",
    "\n",
    "1. Corpus = ['Dogs eat bones', 'Cats eat fish', 'Plants absorb nutrients']\n",
    "\n",
    "   Vocabulary = ['absorb', 'bones', 'cats', 'dogs', 'eat', 'fish', 'nutrients', 'plants']\n",
    "   \n",
    "\n",
    "2. One-hot encoding:\n",
    "\n",
    "$$\\text{absorb} = \\begin{bmatrix}1& 0 & 0& 0& 0& 0& 0& 0\\end{bmatrix}$$\n",
    "$$\\text{bones} = \\begin{bmatrix}0& 1 & 0& 0& 0& 0& 0& 0\\end{bmatrix}$$\n",
    "$$\\text{cats} = \\begin{bmatrix}0& 0 & 1& 0& 0& 0& 0& 0\\end{bmatrix}$$\n",
    "$$\\ldots$$\n",
    "$$\\text{plants} = \\begin{bmatrix}0& 0 & 0& 0& 0& 0& 0& 1\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "3. Create  random $V \\times M$ matrix, where $V$ is vocab size and $M$ is the desired size of the embedding (here, we use 3 for convenience). This is the hidden layer:\n",
    "\n",
    "$$\\text{Hidden Layer} = \\begin{bmatrix}2 & 5 & 5 \\\\7 & 5 & 9 \\\\2 & 5 & 5 \\\\1 & 3 & 2 \\\\0 & 6 & 2 \\\\3 & 5 & 1 \\\\5 & 4 & 8 \\\\3 & 1 & 0\\end{bmatrix}$$\n",
    "\n",
    "  Multiply the one-hot encoding of a word (here, 'plants') by this vector gives a unique word embedding for that word:\n",
    "\n",
    "$$\\text{plants} = \\begin{bmatrix}0& 0 & 0& 0& 0& 0& 0& 1\\end{bmatrix} \\times \\begin{bmatrix}2 & 5 & 5 \\\\7 & 5 & 9 \\\\2 & 5 & 5 \\\\1 & 3 & 2 \\\\0 & 6 & 2 \\\\3 & 5 & 1 \\\\5 & 4 & 8 \\\\3 & 1 & 0\\end{bmatrix} = \\begin{bmatrix}3 & 1& 0\\end{bmatrix}$$\n",
    "\n",
    "4. Next we define a second $M \\times V$ matrix using our word embeddings; this is the output layer weights:\n",
    "\n",
    "$$\\text{Output Layer Weights} = \\begin{bmatrix}2 & 7& 2& 1& 0& 3& 5 & 3 \\\\5& 5& 5& 3& 6& 5& 4& 1 \\\\5& 9& 5& 2& 2& 1& 8& 0\\end{bmatrix}$$\n",
    "\n",
    "5. If we take a word––here, 'plants'––and get the cosine similarity of its word embedding with every column in the weights matrix, we get a similarity score between that word and every other word in the vocalbulary:\n",
    "\n",
    "$$\\text{Similarity scores} = \\begin{bmatrix}3 & 1 &0\\end{bmatrix} \\times \\begin{bmatrix}2 & 7& 2& 1& 0& 3& 5 & 3 \\\\5& 5& 5& 3& 6& 5& 3& 1 \\\\5& 9& 5& 2& 2& 1& 8& 0\\end{bmatrix} = \\begin{bmatrix}0.4 & 0.6 & 0.47 & 0.5 & 0.3 & 0.7 & 0.5 & 1\\end{bmatrix}$$\n",
    "\n",
    "   (For example, $\\text{cosine similarity}(\\begin{bmatrix}3 & 1 &0\\end{bmatrix}, \\begin{bmatrix}2 & 5 & 5\\end{bmatrix}) = 0.4$)\n",
    "\n",
    "6. The softmax function $\\frac{e^i}{\\sum_{j=1}^K e^j}$ converts this matrix of similarity scores into a probability distribution for how likely the word of interest ('plants') will occur with every other word: \n",
    "\n",
    "$$P(plants|x\\in V) = \\begin{bmatrix} 0.10 & 0.12 & 0.11 & 0.11 & 0.09 & 0.14 & 0.11 & 0.19 \\end{bmatrix}$$\n",
    "\n",
    "During training, a `word2vec` model learns the actual probability distribution of 'plants' with respect to all the other words and calculates the difference between this true distribution and the randomly initialsed one. This is called the loss function. It then adjusts the word embeddings to minimise the loss function in an iterative process of updating. Training is complete when any further changes cause the loss function to increase. The word embeddings are retained as a representation of how each word behaves in the corpus. Because similar words will generate similar probability distributions due to co-occurrence, their word embeddings will be similar. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02534c9b",
   "metadata": {},
   "source": [
    "# Example: The King James Bible\n",
    "\n",
    "Let's see what it looks like to train a `word2vec` model with reference to a text––here, the King James version (KJV) of the Bible.\n",
    "\n",
    "![alt text](king-james-bible.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c310b301",
   "metadata": {},
   "source": [
    "## Step 1: Preprocessing our data.\n",
    "\n",
    "For `word2vec` to train, it needs a list of sentences that have been tokenized and lemmatized. This means we must:\n",
    "\n",
    "1. Open our text file\n",
    "2. Split on sentences\n",
    "3. Tokenize and lemmatise our sentences and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Open and read the text file\n",
    "with open('KJV.txt', 'r') as f:\n",
    "    bible = f.read()\n",
    "\n",
    "# Get rid of newline characters and non-ascii gibberish and make everything lowercase\n",
    "bible = bible.encode('ascii', 'ignore')\n",
    "bible = bible.decode()\n",
    "bible = ' '.join(bible.splitlines())\n",
    "bible = bible.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76edebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into sentences and remove trailing and leading whitespace\n",
    "bible = bible.split('.')\n",
    "bible = [i.strip() for i in bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485b482-bb6c-4ca7-9db7-0f17c38b33f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bible[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cebd34e-1de8-44f4-b866-e8facd8bd1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a good tokenixer and lemmatizer\n",
    "\n",
    "def good_tokens(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(i).lower() for i in words]\n",
    "    lemmas = [i for i in lemmas if i not in stops and i not in punct]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize into words\n",
    "\n",
    "bible_tokens = [good_tokens(i) for i in bible]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886ca50",
   "metadata": {},
   "source": [
    "## Step 2: Call and train our `word2vec` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5093ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_model = gensim.models.Word2Vec(bible_tokens, min_count= 20, vector_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65632c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_model.wv.most_similar(['jesus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd9a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_model.wv.doesnt_match(['man', 'woman', 'child', 'horse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c253cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = bible_model.wv.index_to_key\n",
    "vectors = [bible_model.wv[i] for i in vocab]\n",
    "\n",
    "df = pd.DataFrame(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [str(i) for i in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc30fa",
   "metadata": {},
   "source": [
    "## Step 3: Visualising our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the dimensionality of the data so we can plot it\n",
    "\n",
    "pca_1 = PCA(n_components = 3)\n",
    "comps_1 = pca_1.fit_transform(df)\n",
    "pc_df_1 = pd.DataFrame(data = comps_1, columns = ['PC '+str(i) for i in range(1, comps_1.shape[1]+1)])\n",
    "df = pd.concat([df, pc_df_1], axis = 1)\n",
    "df.index = vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbaf527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster our data to see what groups together\n",
    "\n",
    "kmeans = KMeans(n_clusters=20, random_state=0, n_init=\"auto\").fit(df)\n",
    "df['clusters_knn'] = [str(i) for i in kmeans.labels_]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df, x='PC 1', y='PC 2', z='PC 3',\n",
    "              color='clusters_knn', hover_data = [df.index])\n",
    "\n",
    "fig.update_traces(marker=dict(size = 5, line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366db45-cf23-47a0-aa9e-d2891233251e",
   "metadata": {},
   "source": [
    "## Use a pretrained model\n",
    "\n",
    "Training an accurate model is time consuming and computationally intensive. Many NLP packages come with pertrained models already installed. Gensim, for example, has a pretained model based on Google news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c0111-40c7-4a6a-a2a4-4728f8ab4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c18867",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('UK', 'France')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36765c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('UK', 'Mars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = model.index_to_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample = random.choices(vocab, k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b29c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
